{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "import datetime\n",
    "from collections import Counter\n",
    "from sklearn import svm\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path to import modules from src\n",
    "rpath = os.path.abspath('../../week0_network_analysis_project/')\n",
    "if rpath not in sys.path:\n",
    "    sys.path.insert(0, rpath)\n",
    "\n",
    "from src.loader import SlackDataLoader\n",
    "import src.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns we can get from a slack message<br>\n",
    "\n",
    "message_type, message_content, sender_id, time_sent, message_distribution, time_thread_start, reply_count, reply_user_count, time_thread_end, reply_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a single slack message, we can get <br>\n",
    "\n",
    "1. The message<br>\n",
    "2. Type (message, file, link, etc)<br>\n",
    "3. The sender_id (assigned by slack)<br>\n",
    "4. The time the message was sent<br>\n",
    "5. The team (i don't know what that is now)<br>\n",
    "6. The type of the message (broadcast message, inhouse, just messgae)<br>\n",
    "7. The thread the message generated (from here we can go):<br>\n",
    "    7.1 Text/content of the message<br>\n",
    "    7.2 The thread time of the message<br>\n",
    "    7.3 The thread count (reply count)<br>\n",
    "    7.4 The number of user that reply the message (count of users that participated in the thread)<br>\n",
    "    7.5 The time the last thread message was sent <br>\n",
    "    7.6 The users that participated in the thread (their ids are stored as well)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine all json file in all-weeks8-9\n",
    "# def slack_parser(path_channel):\n",
    "#     \"\"\" parse slack data to extract useful informations from the json file\n",
    "#         step of execution\n",
    "#         1. Import the required modules\n",
    "#         2. read all json file from the provided path\n",
    "#         3. combine all json files in the provided path\n",
    "#         4. extract all required informations from the slack data\n",
    "#         5. convert to dataframe and merge all\n",
    "#         6. reset the index and return dataframe\n",
    "#     \"\"\"\n",
    "\n",
    "#     # specify path to get json files\n",
    "#     combined = []\n",
    "#     for json_file in glob.glob(f\"{path_channel}*.json\"):\n",
    "#         with open(json_file, 'r', encoding=\"utf8\") as slack_data:\n",
    "#             combined.append(slack_data)\n",
    "\n",
    "#     # loop through all json files and extract required informations\n",
    "#     dflist = []\n",
    "#     for slack_data in combined:\n",
    "\n",
    "#         msg_type, msg_content, sender_id, time_msg, msg_dist, time_thread_st, reply_users, \\\n",
    "#         reply_count, reply_users_count, tm_thread_end = [],[],[],[],[],[],[],[],[],[]\n",
    "\n",
    "#         for row in slack_data:\n",
    "#             if 'bot_id' in row.keys():\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 msg_type.append(row['type'])\n",
    "#                 msg_content.append(row['text'])\n",
    "#                 if 'user_profile' in row.keys(): sender_id.append(row['user_profile']['real_name'])\n",
    "#                 else: sender_id.append('Not provided')\n",
    "#                 time_msg.append(row['ts'])\n",
    "#                 if 'blocks' in row.keys() and len(row['blocks'][0]['elements'][0]['elements']) != 0 :\n",
    "#                      msg_dist.append(row['blocks'][0]['elements'][0]['elements'][0]['type'])\n",
    "#                 else: msg_dist.append('reshared')\n",
    "#                 if 'thread_ts' in row.keys():\n",
    "#                     time_thread_st.append(row['thread_ts'])\n",
    "#                 else:\n",
    "#                     time_thread_st.append(0)\n",
    "#                 if 'reply_users' in row.keys(): reply_users.append(\",\".join(row['reply_users'])) \n",
    "#                 else:    reply_users.append(0)\n",
    "#                 if 'reply_count' in row.keys():\n",
    "#                     reply_count.append(row['reply_count'])\n",
    "#                     reply_users_count.append(row['reply_users_count'])\n",
    "#                     tm_thread_end.append(row['latest_reply'])\n",
    "#                 else:\n",
    "#                     reply_count.append(0)\n",
    "#                     reply_users_count.append(0)\n",
    "#                     tm_thread_end.append(0)\n",
    "#         data = zip(msg_type, msg_content, sender_id, time_msg, msg_dist, time_thread_st,\n",
    "#          reply_count, reply_users_count, reply_users, tm_thread_end)\n",
    "#         columns = ['msg_type', 'msg_content', 'sender_name', 'msg_sent_time', 'msg_dist_type',\n",
    "#          'time_thread_start', 'reply_count', 'reply_users_count', 'reply_users', 'tm_thread_end']\n",
    "\n",
    "#         df = pd.DataFrame(data=data, columns=columns)\n",
    "#         df = df[df['sender_name'] != 'Not provided']\n",
    "#         dflist.append(df)\n",
    "\n",
    "#     dfall = pd.concat(dflist, ignore_index=True)\n",
    "#     dfall['channel'] = path_channel.split('/')[-1].split('.')[0]        \n",
    "#     dfall = dfall.reset_index(drop=True)\n",
    "    \n",
    "#     return dfall\n",
    "\n",
    "\n",
    "# def parse_slack_reaction(path, channel):\n",
    "#     \"\"\"get reactions\"\"\"\n",
    "#     dfall_reaction = pd.DataFrame()\n",
    "#     combined = []\n",
    "#     for json_file in glob.glob(f\"{path}*.json\"):\n",
    "#         with open(json_file, 'r') as slack_data:\n",
    "#             combined.append(slack_data)\n",
    "\n",
    "#     reaction_name, reaction_count, reaction_users, msg, user_id = [], [], [], [], []\n",
    "\n",
    "#     for k in combined:\n",
    "#         slack_data = json.load(open(k.name, 'r', encoding=\"utf-8\"))\n",
    "        \n",
    "#         for i_count, i in enumerate(slack_data):\n",
    "#             if 'reactions' in i.keys():\n",
    "#                 for j in range(len(i['reactions'])):\n",
    "#                     msg.append(i['text'])\n",
    "#                     user_id.append(i['user'])\n",
    "#                     reaction_name.append(i['reactions'][j]['name'])\n",
    "#                     reaction_count.append(i['reactions'][j]['count'])\n",
    "#                     reaction_users.append(\",\".join(i['reactions'][j]['users']))\n",
    "                \n",
    "#     data_reaction = zip(reaction_name, reaction_count, reaction_users, msg, user_id)\n",
    "#     columns_reaction = ['reaction_name', 'reaction_count', 'reaction_users_count', 'message', 'user_id']\n",
    "#     df_reaction = pd.DataFrame(data=data_reaction, columns=columns_reaction)\n",
    "#     df_reaction['channel'] = channel\n",
    "#     return df_reaction\n",
    "\n",
    "# def get_community_participation(path):\n",
    "#     \"\"\" specify path to get json files\"\"\"\n",
    "#     combined = []\n",
    "#     comm_dict = {}\n",
    "#     for json_file in glob.glob(f\"{path}*.json\"):\n",
    "#         with open(json_file, 'r') as slack_data:\n",
    "#             combined.append(slack_data)\n",
    "#     # print(f\"Total json files is {len(combined)}\")\n",
    "#     for i in combined:\n",
    "#         a = json.load(open(i.name, 'r', encoding='utf-8'))\n",
    "\n",
    "#         for msg in a:\n",
    "#             if 'replies' in msg.keys():\n",
    "#                 for i in msg['replies']:\n",
    "#                     comm_dict[i['user']] = comm_dict.get(i['user'], 0)+1\n",
    "#     return comm_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_2_timestamp(column, data):\n",
    "#     \"\"\"convert from unix time to readable timestamp\n",
    "#         args: column: columns that needs to be converted to timestamp\n",
    "#                 data: data that has the specified column\n",
    "#     \"\"\"\n",
    "#     if column in data.columns.values:\n",
    "#         timestamp_ = []\n",
    "#         for time_unix in data[column]:\n",
    "#             if time_unix == 0:\n",
    "#                 timestamp_.append(0)\n",
    "#             else:\n",
    "#                 a = datetime.datetime.fromtimestamp(float(time_unix))\n",
    "#                 timestamp_.append(a.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "#         return timestamp_\n",
    "#     else: \n",
    "#         print(f\"{column} not in data\")\n",
    "\n",
    "# def get_tagged_users(df):\n",
    "#     \"\"\"get all @ in the messages\"\"\"\n",
    "\n",
    "#     return df['msg_content'].map(lambda x: re.findall(r'@U\\w+', x))\n",
    "\n",
    "\n",
    "    \n",
    "# def map_userid_2_realname(user_profile: dict, comm_dict: dict, plot=False):\n",
    "#     \"\"\"\n",
    "#     map slack_id to realnames\n",
    "#     user_profile: a dictionary that contains users info such as real_names\n",
    "#     comm_dict: a dictionary that contains slack_id and total_message sent by that slack_id\n",
    "#     \"\"\"\n",
    "#     user_dict = {} # to store the id\n",
    "#     real_name = [] # to store the real name\n",
    "#     ac_comm_dict = {} # to store the mapping\n",
    "#     count = 0\n",
    "#     # collect all the real names\n",
    "#     for i in range(len(user_profile['profile'])):\n",
    "#         real_name.append(dict(user_profile['profile'])[i]['real_name'])\n",
    "\n",
    "#     # loop the slack ids\n",
    "#     for i in user_profile['id']:\n",
    "#         user_dict[i] = real_name[count]\n",
    "#         count += 1\n",
    "\n",
    "#     # to store mapping\n",
    "#     for i in comm_dict:\n",
    "#         if i in user_dict:\n",
    "#             ac_comm_dict[user_dict[i]] = comm_dict[i]\n",
    "\n",
    "#     ac_comm_dict = pd.DataFrame(data= zip(ac_comm_dict.keys(), ac_comm_dict.values()),\n",
    "#     columns=['LearnerName', '# of Msg sent in Threads']).sort_values(by='# of Msg sent in Threads', ascending=False)\n",
    "    \n",
    "#     if plot:\n",
    "#         ac_comm_dict.plot.bar(figsize=(15, 7.5), x='LearnerName', y='# of Msg sent in Threads')\n",
    "#         plt.title('Student based on Message sent in thread', size=20)\n",
    "        \n",
    "#     return ac_comm_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_20_user(data, channel='Random'):\n",
    "    \"\"\"get user with the highest number of message sent to any channel\"\"\"\n",
    "\n",
    "    data['sender_name'].value_counts()[:20].plot.bar(figsize=(15, 7.5))\n",
    "    plt.title(f'Top 20 Message Senders in #{channel} channels', size=15, fontweight='bold')\n",
    "    plt.xlabel(\"Sender Name\", size=18); plt.ylabel(\"Frequency\", size=14);\n",
    "    plt.xticks(size=12); plt.yticks(size=12);\n",
    "    plt.show()\n",
    "\n",
    "    data['sender_name'].value_counts()[-10:].plot.bar(figsize=(15, 7.5))\n",
    "    plt.title(f'Bottom 10 Message Senders in #{channel} channels', size=15, fontweight='bold')\n",
    "    plt.xlabel(\"Sender Name\", size=18); plt.ylabel(\"Frequency\", size=14);\n",
    "    plt.xticks(size=12); plt.yticks(size=12);\n",
    "    plt.show()\n",
    "\n",
    "def draw_avg_reply_count(data, channel='Random'):\n",
    "    \"\"\"who commands many reply?\"\"\"\n",
    "\n",
    "    data.groupby('sender_name')['reply_count'].mean().sort_values(ascending=False)[:20]\\\n",
    "        .plot(kind='bar', figsize=(15,7.5));\n",
    "    plt.title(f'Average Number of reply count per Sender in #{channel}', size=20, fontweight='bold')\n",
    "    plt.xlabel(\"Sender Name\", size=18); plt.ylabel(\"Frequency\", size=18);\n",
    "    plt.xticks(size=14); plt.yticks(size=14);\n",
    "    plt.show()\n",
    "\n",
    "def draw_avg_reply_users_count(data, channel='Random'):\n",
    "    \"\"\"who commands many user reply?\"\"\"\n",
    "\n",
    "    data.groupby('sender_name')['reply_users_count'].mean().sort_values(ascending=False)[:20].plot(kind='bar',\n",
    "     figsize=(15,7.5));\n",
    "    plt.title(f'Average Number of reply user count per Sender in #{channel}', size=20, fontweight='bold')\n",
    "    plt.xlabel(\"Sender Name\", size=18); plt.ylabel(\"Frequency\", size=18);\n",
    "    plt.xticks(size=14); plt.yticks(size=14);\n",
    "    plt.show()\n",
    "\n",
    "def draw_wordcloud(msg_content, week):    \n",
    "    # word cloud visualization\n",
    "    allWords = ' '.join([twts for twts in msg_content])\n",
    "    wordCloud = WordCloud(background_color='#975429', width=500, height=300, random_state=21, max_words=500, mode='RGBA',\n",
    "                            max_font_size=140, stopwords=stopwords.words('english')).generate(allWords)\n",
    "    plt.figure(figsize=(15, 7.5))\n",
    "    plt.imshow(wordCloud, interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.title(f'WordCloud for {week}', size=30)\n",
    "    plt.show()\n",
    "\n",
    "def draw_user_reaction(data, channel='General'):\n",
    "    data.groupby('sender_name')[['reply_count', 'reply_users_count']].sum()\\\n",
    "        .sort_values(by='reply_count',ascending=False)[:10].plot(kind='bar', figsize=(15, 7.5))\n",
    "    plt.title(f'User with the most reaction in #{channel}', size=25);\n",
    "    plt.xlabel(\"Sender Name\", size=18); plt.ylabel(\"Frequency\", size=18);\n",
    "    plt.xticks(size=14); plt.yticks(size=14);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insight Extraction\n",
    "\n",
    "Below are some useful questions to answer. Feel free to explore to answer other interesting questions that may be of help to get insight about student's behaviour, need, and future performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum Essential To Do: Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Who are the top and bottom 10  users by:\n",
    "    # Reaction count?\n",
    "\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "data_path = os.path.join(rpath, \"data/anonymized\")\n",
    "data_loader = SlackDataLoader(data_path)\n",
    "directories = [d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))]\n",
    "channel_name = 'all-week8'\n",
    "\n",
    "dataframe_by_directory = [\n",
    "    data_loader.parse_slack_reaction(os.path.join(data_path, directory), channel_name).assign(directory=directory)\n",
    "    for directory in directories\n",
    "]\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "df_combined = reduce(lambda df1, df2: pd.concat([df1, df2], ignore_index=True), dataframe_by_directory)\n",
    "\n",
    "\n",
    "reaction_df = pd.concat(dataframe_by_directory, ignore_index=True)\n",
    "\n",
    "users_df= pd.DataFrame(data_loader.get_users())\n",
    "users_df.rename(columns={'id':'user_id'}, inplace = True)\n",
    "\n",
    "reactions_merged = pd.merge(reaction_df, users_df, on='user_id', how='outer')\n",
    "\n",
    "\n",
    "top_10_reactions = reactions_merged.groupby('real_name')['reaction_count'].sum().reset_index(name='total_reaction_count').sort_values(by='total_reaction_count', ascending=False).head(10)\n",
    "\n",
    "bottom_10_reactions = reactions_merged.groupby('real_name')['reaction_count'].sum().reset_index(name='total_reaction_count').sort_values(by='total_reaction_count', ascending=True).head(10)\n",
    "\n",
    "print(top_10_reactions)\n",
    "print(bottom_10_reactions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Who are the top and bottom 10  users by:\n",
    "        # Reply count? \n",
    "        # Mention? \n",
    "        # Message count? \n",
    "\n",
    "data_path = os.path.join(rpath, \"data/anonymized\")\n",
    "data_loader = SlackDataLoader(data_directory)\n",
    "folders = [d for d in os.listdir(data_directory) if os.path.isdir(os.path.join(data_directory, d))]\n",
    "\n",
    "dataframes_by_folder = [data_loader.slack_parser(os.path.join(data_path, folder)).assign(folder=folder) for folder in folders]\n",
    "\n",
    "parsed_data = pd.concat(dataframes_by_folder, ignore_index=True)\n",
    "parsed_data.head(5)\n",
    "\n",
    "reply_count_by_message = parsed_data.groupby('msg_content')['reply_count'].sum().reset_index(name='Reply count')\n",
    "print(reply_count_by_message)\n",
    "\n",
    "message_count_by_user = parsed_data.groupby('sender_name').size().reset_index(name='Message count')\n",
    "\n",
    "mention_count_by_user = parsed_data['msg_content'].str.count('@\\w+').groupby(parsed_data['sender_name']).sum().reset_index(name='Mention count')\n",
    "\n",
    "reply_count_by_user = parsed_data.groupby('sender_name')['reply_count'].sum().reset_index(name='Reply count')\n",
    "\n",
    "user_stats = pd.merge(message_count_by_user, mention_count_by_user, on='sender_name', how='outer')\n",
    "user_stats = pd.merge(user_stats, reply_count_by_user, on='sender_name', how='outer')\n",
    "\n",
    "user_stats = user_stats.fillna(0)\n",
    "\n",
    "user_stats['Total count'] = user_stats['Message count'] + user_stats['Mention count'] + user_stats['Reply count']\n",
    "\n",
    "top_10_reply = user_stats.sort_values(by='Reply count', ascending=False).head(10)\n",
    "bottom_10_reply = user_stats.sort_values(by='Reply count', ascending=True).head(10)\n",
    "\n",
    "top_10_mention = user_stats.sort_values(by='Mention count', ascending=False).head(10)\n",
    "bottom_10_mention = user_stats.sort_values(by='Mention count', ascending=True).head(10)\n",
    "\n",
    "top_10_message = user_stats.sort_values(by='Message count', ascending=False).head(10)\n",
    "bottom_10_message = user_stats.sort_values(by='Message count', ascending=True).head(10)\n",
    "\n",
    "print(\"Top 10 Users by Reply Count:\")\n",
    "print(top_10_reply[['sender_name', 'Reply count']])\n",
    "\n",
    "print(\"\\nBottom 10 Users by Reply Count:\")\n",
    "print(bottom_10_reply[['sender_name', 'Reply count']])\n",
    "\n",
    "print(\"\\nTop 10 Users by Mention Count:\")\n",
    "print(top_10_mention[['sender_name', 'Mention count']])\n",
    "\n",
    "print(\"\\nBottom 10 Users by Mention Count:\")\n",
    "print(bottom_10_mention[['sender_name', 'Mention count']])\n",
    "\n",
    "print(\"\\nTop 10 Users by Message Count:\")\n",
    "print(top_10_message[['sender_name', 'Message count']])\n",
    "\n",
    "print(\"\\nBottom 10 Users by Message Count:\")\n",
    "print(bottom_10_message[['sender_name', 'Message count']])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading channels: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Top 10 Messages by Reply Count:\n",
      "                                             msg_content  Reply count\n",
      "2304           <@u03v1am5tfa> how many crushes you have?           75\n",
      "6051   first impression: people think i am older than...           63\n",
      "476    1. how do i find my jupyter notebook file in m...           61\n",
      "7475   how many here follow football so much? i think...           57\n",
      "5915                     ethiopian new year loading ....           54\n",
      "6775   hello\\n1. <@u03uvhcv6kb>\\n2. <@u03ug5vfn03>\\n3...           45\n",
      "2151   <@u03uur571a5> solutions to funding account an...           44\n",
      "13902  we had a plan to meet today 2:00 eat at friend...           36\n",
      "120    *<!here> reminder*:timer_clock:\\n\\n*please not...           33\n",
      "12960  the debate was great. we <@u03u1j51vfz>, <@u03...           31\n",
      "\n",
      "Top 10 Messages by Reaction Count:\n",
      "                                             msg_content  Reaction count\n",
      "12960  the debate was great. we <@u03u1j51vfz>, <@u03...              12\n",
      "10156  let's follow each other. drop your link in thi...              11\n",
      "1029   <!channel> do we have people who are in the sa...              11\n",
      "6296                                    good morning all              10\n",
      "262    *here are some other stories from people that ...              10\n",
      "7475   how many here follow football so much? i think...              10\n",
      "1189   <!here>, aws instance is being setup for every...               8\n",
      "4096   _interpreters study group_\\nhello, i am starti...               8\n",
      "14501    which teams are we supporting in the world cup?               8\n",
      "11019    nothing. my partner deserves to know everything               8\n",
      "\n",
      "Top 10 Messages by Mention Count:\n",
      "                                             msg_content  Mention count\n",
      "7614   i am grateful for <@u03u93gnnvb>, <@u03uun8m4r...             25\n",
      "10079  knowing <!here> is just great one way or the o...             20\n",
      "7617   i am grateful to <@u03u9db7reg> for answering ...             17\n",
      "505    1. work ethic\\n2. how communication and openne...             16\n",
      "2200   <@u03uvhcv6kb> <@u03uup56mdf> <@u03ujgrn5e0> <...             13\n",
      "12745  thanks to the team <@u03up7v9q57> <@u03ukl27b0...             12\n",
      "10719  news news *<!here>*........:drum_with_drumstic...              9\n",
      "2201   <@u03uvhcv6kb> <@u03v61vgqg0> <@u03ujgrn5e0> <...              8\n",
      "6775   hello\\n1. <@u03uvhcv6kb>\\n2. <@u03ug5vfn03>\\n3...              8\n",
      "6607    happy birthday <@u03ug4q7v42> and <@u03ug32j3pc>              8\n"
     ]
    }
   ],
   "source": [
    "#What are the top 10 messages by \n",
    "    # Replies?\n",
    "    # Reactions?\n",
    "    # Mentions?\n",
    "data_path = os.path.join(rpath, \"data/anonymized\")\n",
    "data_loader = SlackDataLoader(data_path)\n",
    "folders = [d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))]\n",
    "\n",
    "dataframes_by_folder = [data_loader.slack_parser(os.path.join(data_path, folder)).assign(folder=folder) for folder in folders]\n",
    "\n",
    "parsed_data = pd.concat(dataframes_by_folder, ignore_index=True)\n",
    "parsed_data.head(5)\n",
    "\n",
    "reply_count_by_message = parsed_data.groupby('msg_content')['reply_count'].sum().reset_index(name='Reply count')\n",
    "\n",
    "reaction_count_by_message = parsed_data.groupby('msg_content')['reply_users_count'].sum().reset_index(name='Reaction count')\n",
    "\n",
    "mention_count_by_message = parsed_data['msg_content'].str.count('@\\w+').groupby(parsed_data['msg_content']).sum().reset_index(name='Mention count')\n",
    "\n",
    "message_stats = pd.merge(reply_count_by_message, reaction_count_by_message, on='msg_content', how='outer')\n",
    "message_stats = pd.merge(message_stats, mention_count_by_message, on='msg_content', how='outer')\n",
    "\n",
    "message_stats = message_stats.fillna(0)\n",
    "\n",
    "message_stats['Total count'] = message_stats['Reply count'] + message_stats['Reaction count'] + message_stats['Mention count']\n",
    "\n",
    "top_10_reply_message = message_stats.sort_values(by='Reply count', ascending=False).head(10)\n",
    "top_10_reaction_message = message_stats.sort_values(by='Reaction count', ascending=False).head(10)\n",
    "top_10_mention_message = message_stats.sort_values(by='Mention count', ascending=False).head(10)\n",
    "\n",
    "print(\"\\nTop 10 Messages by Reply Count:\")\n",
    "print(top_10_reply_message[['msg_content', 'Reply count']])\n",
    "\n",
    "print(\"\\nTop 10 Messages by Reaction Count:\")\n",
    "print(top_10_reaction_message[['msg_content', 'Reaction count']])\n",
    "\n",
    "print(\"\\nTop 10 Messages by Mention Count:\")\n",
    "print(top_10_mention_message[['msg_content', 'Mention count']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which channel has the highest activity? \n",
    "# Which channel appears at the right top corner when you plot a 2D scatter plot where x-axis is the number of messages in the channel,\n",
    "# y-axis is the sum of number of replies and reactions, and the color representing channels?\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data_path = os.path.join(rpath, \"data/anonymized\")\n",
    "data_loader = SlackDataLoader(data_path)\n",
    "folders = [d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))]\n",
    "\n",
    "dataframes_by_folder = [data_loader.slack_parser(os.path.join(data_path, folder)).assign(folder=folder) for folder in folders]\n",
    "\n",
    "parsed_data = pd.concat(dataframes_by_folder, ignore_index=True)\n",
    "parsed_data.head(5)\n",
    "\n",
    "message_count_by_channel = parsed_data.groupby('channel')['msg_content'].count().reset_index(name='Message count')\n",
    "activity_by_channel = parsed_data.groupby('channel')[['reply_count', 'reply_users_count']].sum().reset_index()\n",
    "activity_by_channel['Total activity'] = activity_by_channel['reply_count'] + activity_by_channel['reply_users_count']\n",
    "channel_stats = pd.merge(message_count_by_channel, activity_by_channel, on='channel', how='outer')\n",
    "channel_stats = channel_stats.fillna(0)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter_plot = sns.scatterplot(x='Message count', y='Total activity', hue='channel', data=channel_stats, s=100)\n",
    "right_top_channel = channel_stats.loc[channel_stats['Total activity'].idxmax()]\n",
    "scatter_plot.annotate(right_top_channel['channel'], \n",
    "                      (right_top_channel['Message count'], right_top_channel['Total activity']),\n",
    "                      textcoords=\"offset points\", \n",
    "                      xytext=(-10, 5),\n",
    "                      ha='right')\n",
    "plt.show()\n",
    "\n",
    "most_active_channel = channel_stats.loc[channel_stats['Total activity'].idxmax()]\n",
    "print(f\"The channel with the highest activity is: {most_active_channel['channel']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What fraction of messages are replied within the first 5mins?\n",
    "# Plot a 2D scatter plot such that x-axis is the time difference between the message timestamp and the first reply message, \n",
    "# y-axis is the time of the day (in 24hr format), color representing channels? \n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data_path = os.path.join(rpath, \"data/anonymized\")\n",
    "data_loader = SlackDataLoader(data_path)\n",
    "folders = [d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))]\n",
    "\n",
    "dataframes_by_folder = [data_loader.slack_parser(os.path.join(data_path, folder)).assign(folder=folder) for folder in folders]\n",
    "\n",
    "parsed_data = pd.concat(dataframes_by_folder, ignore_index=True)\n",
    "parsed_data.head(5)\n",
    "# slack_parser_df.head(5)\n",
    "\n",
    "parsed_data['time_thread_start'] = pd.to_numeric(parsed_data['time_thread_start'], errors='coerce')\n",
    "parsed_data['msg_sent_time'] = pd.to_numeric(parsed_data['msg_sent_time'], errors='coerce')\n",
    "\n",
    "parsed_data['time_diff_first_reply'] = (parsed_data['time_thread_start'] - parsed_data['msg_sent_time']) / 60\n",
    "\n",
    "parsed_data['time_of_day'] = pd.to_datetime(parsed_data['msg_sent_time'], unit='s').dt.strftime('%H:%M')\n",
    "\n",
    "scatter_data = parsed_data[['time_diff_first_reply', 'time_of_day', 'channel']]\n",
    "\n",
    "# Plot a 2D scatter plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter_plot = sns.scatterplot(x='time_diff_first_reply', y='time_of_day', hue='channel', data=scatter_data, s=100)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum Essential To Do: Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test for slack_parser method\n",
    "# import unittest\n",
    "# test_path = os.path.join(rpath, \"tests\")\n",
    "\n",
    "# # Load the test suite\n",
    "# loader = unittest.TestLoader()\n",
    "# suite = loader.discover('path/to/tests', pattern='test_*.py')\n",
    "\n",
    "# # Run the tests\n",
    "# runner = unittest.TextTestRunner()\n",
    "# result = runner.run(suite)\n",
    "\n",
    "# # Check the test result\n",
    "# result.wasSuccessful()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
